---
title: "Data Analysis Assignment 3"
header-includes:
   - \usepackage{dcolumn}
output:
  pdf_document: default
  word_document: default
always_allow_html: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(knitr)
library(ggplot2)
library(kableExtra)
library(lattice)
library(dplyr)
library(stargazer)
library(gt)
library(janitor)
library(flextable)
library(magrittr)
library(Hmisc)
library(caret)
library(patchwork)
library(leaps)
library(rms)
library(arm)
library(pROC)
library(e1071)
library(caret)
require(gridExtra)
```

```{r echo = FALSE}
nba <- read.csv("nba_games_stats.csv")
```

```{r echo = FALSE}
# Set factor variables
nba$Home <- factor(nba$Home)
nba$Team <- factor(nba$Team)
nba$WINorLOSS <- factor(nba$WINorLOSS)
# Convert date to the right format
nba$Date <- as.Date(nba$Date, "%Y-%m-%d")
# Also create a binary variable from WINorLOSS.
# This is not always necessary but can be useful
#particularly for R functions that prefer numeric binary variables
#to the original factor variables
nba$Win <- rep(0,nrow(nba))
nba$Win[nba$WINorLOSS=="W"] <- 1
# Charlotte hornets subset
nba_reduced <- nba[nba$Team == "CHO", ]
#100*FieldGoals., Opp.FieldGoals.
nba_reduced$FieldGoals. <- (nba_reduced$FieldGoals.)*100
nba_reduced$Opp.FieldGoals.<-(nba_reduced$Opp.FieldGoals.)*100
# Set aside the 2017/2018 season as your test data
nba_reduced_train <- nba_reduced[nba_reduced$Date < "2017-10-01",]
nba_reduced_test <- nba_reduced[nba_reduced$Date >= "2017-10-01",]
```

# Q1 EDA

```{r echo = FALSE,warning=FALSE, message=FALSE, fig.align='center'}
#boxplot for numerical variables(TeamPoints, FieldGoals.,Assists, Steals, Blocks, OpponentPoints, TotalRebounds, and Turnovers)

p1 <- ggplot(nba_reduced_train,aes(x=TeamPoints, y=WINorLOSS, fill=TeamPoints)) +
  geom_boxplot() + 
  scale_fill_brewer(palette="Reds") +
  labs(title="TeamPoints vs Win/Loss",
       x="TeamPoints",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p2 <- ggplot(nba_reduced_train,aes(x=FieldGoals., y=WINorLOSS, fill=FieldGoals.)) +
  geom_boxplot() + 
  scale_fill_brewer(palette="Reds") +
  labs(title="FieldGoals. vs Win/Loss",
       x="FieldGoals.",y="Win or Loss") +
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p3 <- ggplot(nba_reduced_train,aes(x=Assists, y=WINorLOSS, fill=Assists)) +
  geom_boxplot() + 
  scale_fill_brewer(palette="Reds") +
  labs(title="Assists vs Win/Loss",
       x="Assists",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))


p4 <- ggplot(nba_reduced_train,aes(x=Steals, y=WINorLOSS, fill=Steals)) +
  geom_boxplot() + 
  scale_fill_brewer(palette="Reds") +
  labs(title="Steals vs Win/Loss",
       x="Steals",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p5 <- ggplot(nba_reduced_train,aes(x=Blocks, y=WINorLOSS, fill=Blocks)) +
  geom_boxplot() +
  scale_fill_brewer(palette="Reds") +
  labs(title="Blocks vs Win/Loss",
       x="Blocks",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p6 <- ggplot(nba_reduced_train,aes(x=OpponentPoints, y=WINorLOSS, fill=OpponentPoints)) +
  geom_boxplot() +
  scale_fill_brewer(palette="Blues") +
  labs(title="OpponentPoints vs Win/Loss",
       x="OpponentPoints",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p7 <- ggplot(nba_reduced_train,aes(x=TotalRebounds, y=WINorLOSS, fill=TotalRebounds)) +
  geom_boxplot() +
  scale_fill_brewer(palette="Reds") +
  labs(title="TotalRebounds vs Win/Loss",
       x="TotalRebounds",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p8 <- ggplot(nba_reduced_train,aes(x=Turnovers, y=WINorLOSS, fill=Turnovers)) +
  geom_boxplot() + 
  scale_fill_brewer(palette="Blues") +
  labs(title="Turnovers vs Win/Loss",
       x="Turnovers",y="Win or Loss") + 
  theme_classic() + theme(legend.position="none",plot.title = element_text(size = 9), axis.title = element_text(size = 8))

p1+p2+p3+p4+p5+p6+p7+p8+plot_layout(ncol=3)
```
```{r echo = FALSE,warning=FALSE, message=FALSE}
#tables for factor variable Home
t1 <- table(nba_reduced_train[,c("WINorLOSS","Home")])
t2 <- apply(table(nba_reduced_train[,c("WINorLOSS","Home")])/sum(table(nba_reduced_train[,c("WINorLOSS","Home")])),
      2,function(x) x/sum(x)) 
knitr::kable(list(t1, t2))
```

After using boxplots for continuous variables and table for categorical variable, we can tell that the higher the number of total points scored in the game(TeamPoints), the higher the number of field goals made in the game(FieldGoals.), the higher the total number of assists(Assists), the higher the total number of steals (balls stolen from the opposing team while the opposing team has possession) in the game(Steals), the higher the total number of blocks (direct prevention of a made field goal after the ball has been shot by an opposing player) in the game (Blocks) and the higher total number of rebounds grabbed in the game(TotalRebounds), the more likely the tean will win the game, since the median is higher. For the Turnovers( Total number of times the ball was lost back to the opposing team while the team had possession), there seems to have no obvious difference between win and loss. While the lower the number of total points scored by the opposing team in the game(OpponentPoints) and the lower the total number of times the ball was lost back to the opposing team, the more likely the team will lose the game.

The probability of wining changes for the location of the game. When the game is a home game, the team is more likely to win, while when it's a away game, the team is more likely to lose. 

# Q2 
1. We shouldn't include both FieldGoals. and FieldGoals as predictors in the logistic model, since FieldGoadls. contains the information of both FieldGoals and FieldGoalsAttempted.
2. We shouldn't include both FieldGoals. and X3PointShots as predictors in the logistic model, since FieldGoadls. includes 3 point shots, it contains the whole information of X3PointShots.

# Q3
```{r echo = FALSE,warning=FALSE, message=FALSE}
nba_glm <- glm(Win ~ Home + TeamPoints + FieldGoals. + Assists + Steals + Blocks + TotalRebounds + Turnovers, data = nba_reduced_train, family = binomial)
#summary(nba_glm)
knitr::kable(summary(nba_glm)$coefficients,digits = 2)%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```
**Fitted Model**

log(p_win/1-p_win) =$\beta0$ + $\beta1$Home_Home + $\beta2$TeamPoints +$\beta3$ FieldGoals+$\beta4$Assists+ $\beta5$Steals + $\beta6$Blocks + $\beta7$TotalRecounds + $\beta8$Turnovers + $\epsilon$

**log(p_win/1-p_win) = -29.80 + 1.11Home_Home - 0.01TeamPoints + 0.44 FieldGoals. - 0.11Assists + 0.39Steals + 0.04Blocks + 0.28TotalRecounds - 0.17Turnovers + $\epsilon$**

**$\beta_{1}$**: The odds of wining a NBA game when the game is at home are 3.03 times higher than the game is away from home.

**$\beta_{2}$**: As we increase TeamPoints by 1 unit, we increase the log-odds of wining an NBA game by -0.01 (increase the odds for wining an NBA game by a multiplicative effect of 0.99/ the odds of winning an NBA game increase 0.99 times). **--- not significant**

**$\beta_{3}$**: As we increase FieldGoals. by 1 percent, we increase the log-odds of wining an NBA game by 0.44 (increase the odds for wining an NBA game by a multiplicative effect of 1.55/ the odds of winning an NBA game increase 1.55 times).

**$\beta_{4}$**: As we increase Assists by 1 unit, we increase the log-odds of wining an NBA game by -0.11 (increase the odds for wining an NBA game by a multiplicative effect of 0.90/ the odds of winning an NBA game increase 0.90 times).

**$\beta_{5}$**: As we increase Steals by 1 unit, we increase the log-odds of wining an NBA game by 0.39 (increase the odds for wining an NBA game by a multiplicative effect of 1.48/ the odds of winning an NBA game increase 1.48 times).

**$\beta_{6}$**: As we increase Blocks by 1 unit, we increase the log-odds of wining an NBA game by 0.04 (increase the odds for wining an NBA game by a multiplicative effect of 1.04/ the odds of winning an NBA game increase 1.04 times). **--- not significant**

**$\beta_{7}$**: As we increase TotalRebounds by 1 unit, we increase the log-odds of wining an NBA game by 0.28 (increase the odds for wining an NBA game by a multiplicative effect of 1.32/ the odds of winning an NBA game increase 1.32 times).

**$\beta_{8}$**: As we increase TurnOverss by 1 unit, we increase the log-odds of wining an NBA game by -0.17 (increase the odds for wining an NBA game by a multiplicative effect of 0.84/ the odds of winning an NBA game increase 0.84 times).

# Q4
```{r echo = FALSE,warning=FALSE, message=FALSE}
#vif(nba_glm)
knitr::kable(vif(nba_glm),digits = 2)%>% kable_styling(full_width = FALSE,latex_options = c("hold_position"))
```
**Since all VIFs are between 1 and 5(<10), which means they are moderately correlated, we don't need to worry about multicollinearity.**

# Q5
```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=5,fig.height= 3}
Conf_mat <- confusionMatrix(as.factor(ifelse(fitted(nba_glm) >= 0.5, "1","0")),
                            as.factor(nba_reduced_train$Win),positive = "1")
t1 <- Conf_mat$table
t2 <- Conf_mat$overall["Accuracy"];
t3 <- Conf_mat$byClass[c("Sensitivity","Specificity")]

knitr::kable(list(t1, t2,t3))
```

```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=5,fig.height= 3,results='hide',fig.keep='all'}
roc(nba_reduced_train$Win,fitted(nba_glm),plot=T,print.thres="best",legacy.axes=T,
    print.auc =T,col="red3")
```
The accuracy of this model is **0.813**. 

**AUC:0.897**


# Q6

```{r echo = FALSE,warning=FALSE, message=FALSE}
nba_glm_new <- glm(Win ~ Home + TeamPoints + FieldGoals. + Assists + Steals + Blocks + TotalRebounds + Turnovers + Opp.FieldGoals. + Opp.TotalRebounds + Opp.TotalFouls + Opp.Turnovers, data = nba_reduced_train, family = binomial)
#summary(nba_glm_new)
knitr::kable(summary(nba_glm_new)$coefficients,digits = 2)%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```

**log(p_win/1-p_win) = 9.19 + 1.53Home_Home + 0.14TeamPoints + 0.40 FieldGoals - 0.01Assists + 0.35Steals - 0.09Blocks + 0.17TotalRebounds - 0.55Turnovers - 0.86Opp.FieldGoals. - 0.36Opp.TotalRebounds + 0.10Opp.TotalFouls + 0.61Opp.Turnovers + $\epsilon$**

Significant coefficients includes $\beta_{1}$, $\beta_{2}$, $\beta_{3}$, $\beta_{8}$, $\beta_{9}$, $\beta_{10}$, and $\beta_{12}$

**$\beta_{1}$**: The odds of wining a NBA game when the game is at home are 4.62 times higher than the game is away from home.

**$\beta_{2}$**: As we increase TeamPoints by 1 unit, we increase the log-odds of wining an NBA game by 0.14 (increase the odds for wining an NBA game by a multiplicative effect of 1.15/ the odds of winning an NBA game increase 1.15 times). 

**$\beta_{3}$**: As we increase FieldGoals. by 1 percent, we increase the log-odds of wining an NBA game by 0.40 (increase the odds for wining an NBA game by a multiplicative effect of 1.49/ the odds of winning an NBA game increase 1.49 times).

**$\beta_{8}$**: As we increase TurnOvers by 1 unit, we increase the log-odds of wining an NBA game by -0.55 (increase the odds for wining an NBA game by a multiplicative effect of 0.58/ the odds of winning an NBA game increase 0.58 times).

**$\beta_{9}$**: As we increase Opp.FieldGoals. by 1 percent, we increase the log-odds of wining an NBA game by -0.86 (increase the odds for wining an NBA game by a multiplicative effect of 0.42/ the odds of winning an NBA game increase 0.42 times).

**$\beta_{10}$**: As we increase Opp.TotalRebounds by 1 unit, we increase the log-odds of wining an NBA game by -0.36 (increase the odds for wining an NBA game by a multiplicative effect of 0.70/ the odds of winning an NBA game increase 0.70 times).

**$\beta_{12}$**: As we increase Opp.Turnovers by 1 unit, we increase the log-odds of wining an NBA game by 0.61 (increase the odds for wining an NBA game by a multiplicative effect of 1.84/ the odds of winning an NBA game increase 1.84 times).

# Q7
```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=4,fig.height= 2.5}
Conf_mat <- confusionMatrix(as.factor(ifelse(fitted(nba_glm_new) >= 0.5, "1","0")),
                            as.factor(nba_reduced_train$Win),positive = "1")
t1.1 <- Conf_mat$table
t2.1 <- Conf_mat$overall["Accuracy"];
t3.1 <- Conf_mat$byClass[c("Sensitivity","Specificity")]

knitr::kable(list(t1.1, t2.1,t3.1))%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```

```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=5,fig.height= 3,results='hide',fig.keep='all'}
roc(nba_reduced_train$Win,fitted(nba_glm_new),plot=T,print.thres="best",legacy.axes=T,
    print.auc =T,col="red3")
```
The accuracy of this model is **0.927**. 

**AUC for model that includes Opp.FieldGoals., Opp.TotalRebounds, Opp.TotalFouls, and Opp.Turnovers as predictors is higher(0.983 > 0.897), the accuracy is also higher(0.927>0.813). Therefore, this new model is better.**

# Q8
```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=5,fig.height= 3,fig.keep='all'}
pred <- predict(nba_glm_new, nba_reduced_test, type = "response")
pred_new <- as.factor(ifelse(pred >=0.5,'1','0'))

Conf_mat <- confusionMatrix(pred_new,
                            as.factor(nba_reduced_test$Win),positive = "1")
t1<-Conf_mat$table
t2<-Conf_mat$overall["Accuracy"];
t3<-Conf_mat$byClass[c("Sensitivity","Specificity")]

knitr::kable(list(t1, t2,t3))%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```

The accuracy of this model is **0.866**. The sensitivity of this model is **0.944** and the specificity is **0.804**(1-specificity is 0.196). Therefore, the model do well in predicting data for the 2017/2018 season.


# Q9
```{r echo = FALSE,warning=FALSE, message=FALSE}
nba_glm_new_1 <- glm(Win ~ Home + TeamPoints + FieldGoals. + Assists + Steals + Blocks + TotalRebounds + Turnovers + Opp.FieldGoals. + Opp.TotalRebounds + Opp.TotalFouls + Opp.Turnovers + Opp.Assists +  Opp.Blocks, data = nba_reduced_train, family = binomial)
#summary(nba_glm_new_1)
#knitr::kable(summary(nba_glm_new_1)$coefficients,digits = 2)%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
#anova(nba_glm_new,nba_glm_new_1, test = 'Chisq')
knitr::kable(anova(nba_glm_new,nba_glm_new_1,test = 'Chisq'),digits = 2)%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```
\
```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=5,fig.height= 3,fig.keep='all'}
pred1 <- predict(nba_glm_new_1, nba_reduced_test, type = "response")
pred_new_1 <- as.factor(ifelse(pred1 >=0.5,'1','0'))

Conf_mat <- confusionMatrix(pred_new_1,
                            as.factor(nba_reduced_test$Win),positive = "1")
t1<-Conf_mat$table
t2<-Conf_mat$overall["Accuracy"];
t3<-Conf_mat$byClass[c("Sensitivity","Specificity")]

knitr::kable(list(t1, t2,t3))%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```

* **P value = 0.46(>0.05)**, which means that there is no statistically significance between the model that including Opp.Assists and Opp.Blocks and the original model. However, When using this model to predict data for the 2017/2018 season, the accuracy is **0.890**(>0.866), the sensitivity is **0.944**, and the specificity is **0.848**(>0.804). Since *accuracy increases and 1-specificity decreases*, including Opp.Assists and Opp.Blocks in the model at the same time improves the model. 

\
```{r echo = FALSE,warning=FALSE, message=FALSE}
nba_glm_new_2 <- glm(Win ~ Home + TeamPoints + FieldGoals. + Assists + Steals + Blocks + TotalRebounds + Turnovers + Opp.FieldGoals. + Opp.TotalRebounds + Opp.TotalFouls + Opp.Turnovers + FreeThrows., data = nba_reduced_train, family = binomial)
#summary(nba_glm_new_2)
#vif(nba_glm_new_2)
#knitr::kable(summary(nba_glm_new_2)$coefficients,digits = 2)%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
#anova(nba_glm_new,nba_glm_new_2, test = 'Chisq')
knitr::kable(anova(nba_glm_new,nba_glm_new_2,test = 'Chisq'),digits = 2)%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```

```{r echo = FALSE,warning=FALSE, message=FALSE,fig.align='center',fig.width=5,fig.height= 3,fig.keep='all'}
pred2 <- predict(nba_glm_new_2, nba_reduced_test, type = "response")
pred_new_2 <- as.factor(ifelse(pred1 >=0.5,'1','0'))

Conf_mat <- confusionMatrix(pred_new_2,
                            as.factor(nba_reduced_test$Win),positive = "1")
t1<-Conf_mat$table
t2<-Conf_mat$overall["Accuracy"];
t3<-Conf_mat$byClass[c("Sensitivity","Specificity")]

knitr::kable(list(t1, t2,t3))%>% kable_styling(position="center",full_width = FALSE,latex_options = c("hold_position"))
```
* **FreeThrows.** may improve our model, because the predictor FieldGoals. includes both number of field goals made and attempted in the game (also includes 3 point shots but not free throws). Though the p=value of the model including FreeThrows. is 0.10, which is not statistically significant enough, when using this model to predict data for the 2017/2018 season, the accuracy is **0.890**(>0.866), the sensitivity is **0.944**, and the specificity is **0.848**(>0.804). Since *accuracy increases and 1-specificity decreases*, including Opp.Assists and Opp.Blocks in the model at the same time improves the model(the model that I selected in Q7). 

# Q10

* Although during exploratory data analysis, for the Turnovers( Total number of times the ball was lost back to the opposing team while the team had possession), there seems to have no obvious difference between win and loss. In each model, the coefficient of Turnovers is statistically significant. 

* In order to have a higher chance of wining, team should try to get higher FieldGoals/FieldGoalsAttempted and lower the total number of times the ball was lost back to the opposing team while the team had possession(Turnovers). In the meantime, if Opp.FieldGoals/Opp.FieldGoalsAttempted is lower, the number of offensive rebounds grabbed by the opposing team in the game is lower, and the higher the total number of times the ball was won back from the opposing team while the opposing team had possession, the higher the possibility the team will win.

\newpage

## Appendix: All code for this report
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```